import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_decision_regions

# Step 1: Create dataset
df = pd.DataFrame()
df['X1'] = [1, 2, 3, 4, 5, 6, 6, 7, 9, 9]
df['X2'] = [5, 3, 6, 8, 1, 9, 5, 8, 9, 2]
df['label'] = [1, 1, 0, 1, 0, 1, 0, 1, 0, 0]
df['weights'] = 1 / df.shape[0]

# üéØ Visualize initial data
plt.figure(figsize=(6, 5))
sns.scatterplot(x='X1', y='X2', hue='label', data=df, palette='Set1', s=80)
plt.title("Initial Data Distribution")
plt.grid(True)
plt.show()

# Step 2: Train first decision stump
x = df[['X1', 'X2']].values
y = df['label'].values
dt1 = DecisionTreeClassifier(max_depth=1)
dt1.fit(x, y)
df['y_pred'] = dt1.predict(x)

# üìä Plot decision region - Tree 1
plt.figure(figsize=(6, 5))
plot_decision_regions(x, y, clf=dt1, legend=2)
plt.title("Decision Region - Tree 1")
plt.grid(True)
plt.show()

# Step 3: Calculate model weight
def calculate_model_weight(error):
    return 0.5 * np.log((1 - error) / error)

error1 = np.sum(df['weights'] * (df['label'] != df['y_pred']))
alpha1 = calculate_model_weight(error1)

# Step 4: Update weights
def update_row_weights(row, alpha):
    if row['label'] == row['y_pred']:
        return row['weights'] * np.exp(-alpha)
    else:
        return row['weights'] * np.exp(alpha)

df['updated_weights'] = df.apply(update_row_weights, axis=1, alpha=alpha1)
df['normalized_weights'] = df['updated_weights'] / df['updated_weights'].sum()
df['cumsum_upper'] = np.cumsum(df['normalized_weights'])
df['cumsum_lower'] = df['cumsum_upper'] - df['normalized_weights']

# Step 5: Resample based on weights
def create_new_dataset(df):
    indices = []
    for _ in range(df.shape[0]):
        a = np.random.random()
        for index, row in df.iterrows():
            if row['cumsum_lower'] < a <= row['cumsum_upper']:
                indices.append(index)
                break
    return indices

index_values = create_new_dataset(df)
second_df = df.iloc[index_values].copy()

# Step 6: Train second decision stump
x2 = second_df[['X1', 'X2']].values
y2 = second_df['label'].values
dt2 = DecisionTreeClassifier(max_depth=1)
dt2.fit(x2, y2)
second_df['y_pred'] = dt2.predict(x2)

# üìä Plot decision region - Tree 2
plt.figure(figsize=(6, 5))
plot_decision_regions(x2, y2, clf=dt2, legend=2)
plt.title("Decision Region - Tree 2")
plt.grid(True)
plt.show()

# Step 7: Calculate second model weight
error2 = np.sum(second_df['weights'] * (second_df['label'] != second_df['y_pred']))
alpha2 = calculate_model_weight(error2)

# Step 8: Train third decision stump on original data
dt3 = DecisionTreeClassifier(max_depth=1)
dt3.fit(x, y)
df['y_pred_3'] = dt3.predict(x)
error3 = np.sum(df['weights'] * (df['label'] != df['y_pred_3']))
alpha3 = calculate_model_weight(error3)

# üìå Print model weights
print("Alpha1:", round(alpha1, 3))
print("Alpha2:", round(alpha2, 3))
print("Alpha3:", round(alpha3, 3))

# Step 9: Ensemble prediction for query point
def ensemble_predict(query):
    pred1 = dt1.predict(query)[0]
    pred2 = dt2.predict(query)[0]
    pred3 = dt3.predict(query)[0]
    vote = alpha1 * (1 if pred1 == 1 else -1) + \
           alpha2 * (1 if pred2 == 1 else -1) + \
           alpha3 * (1 if pred3 == 1 else -1)
    return int(np.sign(vote) > 0)

# üîç Test queries
query1 = np.array([1, 5]).reshape(1, -1)
query2 = np.array([9, 9]).reshape(1, -1)

print("Ensemble prediction for [1,5]:", ensemble_predict(query1))
print("Ensemble prediction for [9,9]:", ensemble_predict(query2))
